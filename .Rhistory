install.packages(c("class", "digest", "doBy", "foreign", "gtools", "lattice", "MASS", "Matrix", "mgcv", "multcomp", "mvtnorm", "nlme", "nnet", "rpart", "slam", "spatial", "survival", "tm"))
## Intelligence Squared US
## 2014-02-25 Brian McInnis
## Objective:  Depict turntaking during debate section (R3)
#  by LIWC function word categories (as percent of statement words)
#  also showing Opening Statement and Closing Statements as
#  baseline.
## Load libraries
rm(list = ls());
require(gdata);
i2us<-read.xls(xls=r'/Users/bjm277/code/intelligence-squared/debateData (2014-02-22).xlsx',
sheet=1);
## Intelligence Squared US
## 2014-02-25 Brian McInnis
## Objective:  Depict turntaking during debate section (R3)
#  by LIWC function word categories (as percent of statement words)
#  also showing Opening Statement and Closing Statements as
#  baseline.
## Load libraries
rm(list = ls());
require(gdata);
i2us<-read.xls(xls='/Users/bjm277/code/intelligence-squared/debateData (2014-02-22).xlsx',
sheet=1);
i2us<-read.xls(xls='/Users/bjm277/code/intelligence-squared/debateData (2014-02-22).xlsx',
sheet=1,
na.strings=c("NA","#DIV/0!"));
i2us<-read.xlsx(xls='/Users/bjm277/code/intelligence-squared/debateData (2014-02-22).xlsx',
sheet=1,
na.strings=c("NA","#DIV/0!"));
require(xlsx);
install.packages("xlsx")
require(xlsx);
i2us<-read.xlsx(file='/Users/bjm277/code/intelligence-squared/debateData (2014-02-22).xlsx',
sheetIndex=1);
i2us.dr<-read.xlsx(
file='/Users/bjm277/code/intelligence-squared/debateData (2014-02-22).xlsx',
sheetName='Results'
);
View(`i2us.dr`)
View(`i2us.dr`)
## Intelligence Squared US (I2US)
## 2014-02-25 Brian McInnis
## Objective:  Depict turntaking during debate section (R3)
#  by LIWC function word categories (as percent of statement words)
#  also showing Opening Statement and Closing Statements as
#  baseline.
## Clear recent variables
rm(list = ls());
## Load libraries
require(xlsx);
## Using XLSX read the I2US results to memory as a dataframe (14 variables, 21 obs.)
i2us.dr<-read.xlsx(
file='/Users/bjm277/code/intelligence-squared/debateData (2014-02-22).xlsx',
sheetName='Results'
);
## Using XLSX read the I2US data to memory as a dataframe (73 variables, 6,024 obs.)
#  This takes a while because the statement strings are very long.
i2us.dd<-read.xlsx(
file='/Users/bjm277/code/intelligence-squared/debateData (2014-02-22).xlsx',
sheetName='DebateData'
);
## Intelligence Squared US (I2US)
## 2014-02-25 Brian McInnis
## Objective:  Depict turntaking during debate section (R3)
#  by LIWC function word categories (as percent of statement words)
#  also showing Opening Statement and Closing Statements as
#  baseline.
## Clear recent variables
rm(list = ls());
## Load libraries
require(xlsx);
## Using XLSX read the I2US results to memory as a dataframe (14 variables, 21 obs.)
i2us.dr<-read.xlsx(
file='/Users/bjm277/code/intelligence-squared/debateData (2014-02-22).xlsx',
sheetName='Results'
);
## Using XLSX read the I2US data to memory as a dataframe (73 variables, 6,024 obs.)
#  This takes a while because the statement strings are very long.
i2us.dd<-read.xlsx(
file='/Users/bjm277/code/intelligence-squared/debateData (2014-02-22).xlsx',
sheetName='DDwoStatement'
);
## Intelligence Squared US (I2US)
## 2014-02-25 Brian McInnis
## Objective:  Depict turntaking during debate section (R3)
#  by LIWC function word categories (as percent of statement words)
#  also showing Opening Statement and Closing Statements as
#  baseline.
## Clear memory
rm(list = ls());
## Install and load any necessary packages
pack <- c('plyr','xlsx','XLConnect','ggplot2','data.table','reshape');
for (p in pack) {
print(p);
if(p %in% rownames(installed.packages()) == FALSE){
install.packages(p);
}
if(p %in% rownames(installed.packages()) == TRUE){
require(p,character.only=TRUE);
}
}
rm(p,pack);
## Create the I2US Environment
setwd('/Users/bjm277/code/intelligence-squared/')
lf<-list.files(pattern = '^i2us[.]rda[ta]*$',
ignore.case = TRUE);
if (length(lf)==0) {
i2us <- new.env();
## Load XLSX libraries
require(xlsx);
options(java.parameters = "-Xmx1024m")
require(XLConnect)
rm(lf);
}else{
load(lf[1]);
rm(lf);
}
## Using XLSX read the I2US results to memory as a dataframe (14 variables, 21 obs.)
if (exists("dr",where=i2us)==FALSE){
dr<-read.xlsx(
file='debateData (2014-02-22).xlsx',
sheetName='Results'
);
assign("dr", dr, envir = i2us);
## Save the i2us environment to the working directory
save(i2us, file = "i2us.RData");
rm(dr);
}
if (exists("dd",where=i2us)==FALSE){
## Using XLSX read the I2US data to memory as a dataframe (73 variables, 6,024 obs.)
#  This takes a while because the statement strings are very long.
dd<-read.xlsx(
file='debateData (2014-02-22).xlsx',
#              sheetName='DDwoStatement'
sheetName='DebateData'
);
assign("dd", dd, envir = i2us);
## Save the i2us environment to the working directory
save(i2us, file = "i2us.RData");
rm(dd);
}
## At this point the i2us environment is loaded, whether by opening
#  the XLSX files and reading them in, or merely reopening the environment
#  from the working directory.  Now, perform some quick checks on the
#  data before producing the LIWC percentages and a ggplot to display them
#  by debate and speaker turn.
attach(i2us);
## Create an outcome variable to indicate which side won the debate
dr$outcome[(dr$poll.delta.against>dr$poll.delta.for)]<-"against";
dr$outcome[(dr$poll.delta.against<dr$poll.delta.for)]<-"for";
dr$outcome[(dr$outcome=="")]<-"undecided";
## Convert LIWC scores to percentages
# (1) Identify all of the LIWC variables of interest
titles <- unique(dd$title)
liwc<-grep(pattern='^liwcScores[.](?!total).*$',
x=names(dd),
ignore.case = TRUE,
perl = TRUE,
value = TRUE);
## LSM:  Linguistic Style Matching
## Based on 9 LIWC:  articles, common adverbs, personal pronouns, indefinite pronouns, prepositions, negations, conjunctions, and quantifiers
liwc<-c("liwcScores.Article","liwcScores.Adverbs","liwcScores.Posemo","liwcScores.Ppron","liwcScores.Ipron","liwcScores.Prep","liwcScores.Negate","liwcScores.Conj","liwcScores.Quant");
basic<-c("line","title","file","section","role","speaker");
totals<-c("liwcScores.total.words","liwcScores.total.sentences");
dd.liwc <- dd[,names(dd) %in% c(basic,liwc,totals)];
## Create the LIWC percentages
#   The function eLIWC creates a rolling average LIWC score, this is so that
#   when calculating the LSM (Linguistic Style Matching), each exchange will
#   represent the average LSM at that point (based on all prior exchanges).
#   Most papers aggregate at the experience or document level, the rolling
#   approach is to eventually see if there's a way to predict an outcome
#   based on early exchanges.
## DDPLY (part of the PLYR package) is used to create by group LIWC calculations.
#   Here LIWC scores are generated by title.
eLIWC <- function(x){
print(x[1,"title"]);
for (i in 1:nrow(x)) {
for (t in liwc){
#print(paste('---',t,sep=""));
x[i,paste("perc.",t,sep="")] <-(sum(x[seq(from=1,to=i),t])/sum(x[seq(from=1,to=i),"liwcScores.total.words"]))*100;
}
}
return(x);
};
dd.liwc<-ddply(dd.liwc,.(title,section,speaker), eLIWC);
dd.liwc<-dd.liwc[with(dd.liwc, order(title, line)), ]
## The ePairing function identifies the next speaker and their role in the debate.
#   Additionally, the speakingOrder variable is created so that summaries (i.e., LSM)
#   can be made based on who is speaking to whom.
## Here DDPLY aggregates the results by title and by section in order to isolate the
#   debate portion of the show.
ePairing <- function(x){
## Return the prior value for each speaker
ids<-c("speaker","role");
for (t in ids){
x[,paste("p.",t,sep="")] <- c(rep(NA,1),head(as.character(x[,t]),-1));
summary(x[3,c(t,paste("p.",t,sep=""))]);
}
x[,"speakingOrder"]<-paste(x$role,x$p.role,sep="<-");
return(x);
}
dd.lsm<-ddply(dd.liwc,.(title,section), ePairing);
## eLSM calculates the LSM
#
eLSM <- function(x){
for (t in c(paste("perc.",liwc,sep=""))){
x[,paste("p.",t,sep="")] <- c(rep(0.0001,1),head(as.numeric(x[,t]),-1));
# (Ireland, 2010)  Add + 0.0001 to the denominator to prevent empty sets
x[,paste("lsm.",t,sep="")] <- as.numeric(1-(abs(x[,paste("p.",t,sep="")] - x[,t])/(x[,paste("p.",t,sep="")] + x[,t] + 0.0001)));
#print(summary(dd.liwc[,c(t,paste("p.",t,sep=""),paste("lsm.",t,sep=""))]));
}
for (i in 1:nrow(x)){
x[i,"lsm"]<-sum(x[i,c(paste("lsm.",paste("perc.",liwc,sep=""),sep=""))])/length(liwc);
}
return(x);
}
dd.lsm<-ddply(dd.lsm,.(title,section,speakingOrder), eLSM);
## Create several GGPLOTs to depict the LSM by utterance during the debate section
for (t in titles){
print(t);
tsub <- subset(dd.lsm,(title==t)&((speakingOrder=="against<-for")|(speakingOrder=="for<-against")|(speakingOrder=="for<-moderator")|(speakingOrder=="against<-moderator"))&((section=="R3")));
lSect <- aggregate(.~section,
data=tsub[,c("line","section")],
FUN=min)
p <- ggplot(data=tsub,
aes(x=line,
colour=factor(speakingOrder),
y=lsm,
group=factor(speakingOrder)))+
labs(title = paste("I2US Debate: ",t,sep=""), x = "By utterance (within Debate Section)", y = "Linguistic Style Matching (LSM)")+
geom_point()+
geom_line();
#    geom_line(aes(x=lSect[(lSect$section=="R3"),"line"], colour="R3"))+
#    geom_line(aes(x=lSect[(lSect$section=="R4"),"line"], colour="R4"))+
#    stat_smooth(method = "lm");
ggsave(plot=p, filename=paste(t,".png",sep=""));
print(p);
}
## Merge the DD with DR
dm<-merge(x=dd.liwc,
y=dr,
by=c("file"));
#####
#detach(i2us);
#save(i2us, file = "i2us.RData");
print("--end--");
View(dd)
save(i2us, file = "i2us.RData");
pack <- c('plyr','xlsx','XLConnect','ggplot2','data.table','reshape', 'topicmodels');
for (p in pack) {
print(p);
if(p %in% rownames(installed.packages()) == FALSE){
install.packages(p);
}
if(p %in% rownames(installed.packages()) == TRUE){
require(p,character.only=TRUE);
}
}
rm(p,pack);
pack <- c('plyr','xlsx','XLConnect','ggplot2','data.table','reshape','tm', 'topicmodels');
for (p in pack) {
print(p);
if(p %in% rownames(installed.packages()) == FALSE){
install.packages(p);
}
if(p %in% rownames(installed.packages()) == TRUE){
require(p,character.only=TRUE);
}
}
rm(p,pack);
names(dd)
dd.tm <- dd[,c("file","line","statement")];
dd.statement <- dd[,c("file","line","statement")];
dd.tm <- DocumentTermMatrix(dd.statement[,"statement"]);
dd.tm <- DocumentTermMatrix(dd.statement);
dd.tm <- DocumentTermMatrix(as.list(dd.statement[,"statement"]));
data("acq")
View(acq)
type(acq)
dd.statement <- DataframeSource(x=dd.statement);
dd.tm <- DocumentTermMatrix(dd.statement);
inspect(dd.statement)
inspect(Corpus(dd.statement))
dd.tm <- DocumentTermMatrix(Corpus(dd.statement));
inspect(dd.tm[1:5,100:105])
dd.tm <- DocumentTermMatrix(Corpus(DataframeSource(x=dd.statement[,'statement'])));
inspect(dd.tm[1:5,100:105])
dd.statement <- dd[,c("file","line","statement")];
#inspect(Corpus(dd.statement))
dd.tm <- DocumentTermMatrix(Corpus(DataframeSource(x=dd.statement[,'statement'])));
inspect(dd.tm[1:5,100:105])
dd.tm <- DocumentTermMatrix(Corpus(DataframeSource(x=dd.statement[,c('statement')])));
inspect(dd.tm[1:5,100:105])
dd.statement <- DataframeSource(x=dd.statement);
View(dd.statement)
dd.statement <- dd[,c("file","line","statement")];
dd.statement <- DataframeSource(x=dd.statement);
View(dd.statement)
dd.tm <- DocumentTermMatrix(Corpus(dd.statement[,"statement"]));
dd.statement <- dd[,c("file","line","statement")];
dd.s <- DataframeSource(x=dd.statement[,"statement"]);
dd.statement <- dd[,c("file","statement")];
dd.statement <- DataframeSource(x=dd.statement);
dd.tm <- DocumentTermMatrix(Corpus(dd.statement));
inspect(dd.tm[1:5,100:105])
findFreqTerms(dd.tm, 5)
dd.statement <- dd[,c("statement")];
dd.statement <- DataframeSource(x=dd.statement);
#inspect(Corpus(dd.statement))
dd.tm <- DocumentTermMatrix(Corpus(dd.statement));
findFreqTerms(dd.tm, 5)
dd.statement <- dd[,c("file","statement")];
dd.statement <- DataframeSource(x=dd.statement);
dd$rNames = paste(dd$file, dd$line, sep="_");
dd.statement <- data.frame(docs=c(dd[,"statement"]),row.names = c(dd$rNames);
dd.statement <- data.frame(docs=c(dd[,"statement"]),row.names = c(dd$rNames));
dd.statement <- DataframeSource(x=dd.statement);
inspect(Corpus(dd.statement))
dd.tm <- DocumentTermMatrix(Corpus(dd.statement))
findFreqTerms(dd.tm, 5);
inspect(Corpus(dd.statement))
dd.statement <- data.frame(docs=dd[,"statement"],row.names = c(dd$rNames));
dd.statement <- DataframeSource(x=dd.statement);
inspect(Corpus(dd.statement))
dd.tm <- DocumentTermMatrix(Corpus(dd.statement));
findFreqTerms(dd.tm, 5);
dd.tm <- DocumentTermMatrix(Corpus(dd.statement),control = list(removePunctuation = TRUE));
findFreqTerms(dd.tm, 5);
dd.tm <- DocumentTermMatrix(Corpus(dd.statement),control = list(removePunctuation = TRUE,
tokenize = strsplit_space_tokenizer,
stemming=TRUE));
findFreqTerms(dd.tm, 5);
findAssocs(dd.tm, "war", 0.8);
findAssocs(dd.tm, "war", 0.1);
findAssocs(dd.tm, "war", 0.5);
ctm <- CTM(dd.tm, k = 2);
dd.statement <- data.frame(docs=dd[(dd$title==titles[1]),"statement"],row.names = c(dd$rNames));
dd.statement <- DataframeSource(x=dd.statement);
#inspect(Corpus(dd.statement))
dd.tm <- DocumentTermMatrix(Corpus(dd.statement),control = list(removePunctuation = TRUE,
tokenize = strsplit_space_tokenizer,
stemming=TRUE));
findFreqTerms(dd.tm, 5);
findAssocs(dd.tm, "war", 0.5);
dd.statement <- data.frame(docs=dd[(dd$title==titles[1]),"statement"],row.names = c(dd$rNames));
dd.statement <- data.frame(docs=dd[(dd$title==titles[1]),"statement"],row.names = c(dd[(dd$title==titles[1]),"rNames"]));
dd.statement <- DataframeSource(x=dd.statement);
#inspect(Corpus(dd.statement))
dd.tm <- DocumentTermMatrix(Corpus(dd.statement),control = list(removePunctuation = TRUE,
tokenize = strsplit_space_tokenizer,
stemming=TRUE));
findFreqTerms(dd.tm, 5);
findAssocs(dd.tm, "war", 0.5);
dd.statement <- DataframeSource(x=dd.statement);
dd.statement <- data.frame(docs=dd[(dd$title==titles[1]),"statement"],row.names = c(dd[(dd$title==titles[1]),"rNames"]));
dd.statement <- DataframeSource(x=dd.statement);
dd.tm <- DocumentTermMatrix(Corpus(dd.statement),control = list(removePunctuation = TRUE,
tokenize = strsplit_space_tokenizer,
stemming=TRUE));
dd.tm <- DocumentTermMatrix(Corpus(dd.statement),control = list(removePunctuation = TRUE,
stemming=TRUE));
dd.tm <- DocumentTermMatrix(Corpus(dd.statement),control = list(removePunctuation = TRUE));
findFreqTerms(dd.tm, 5);
findAssocs(dd.tm, "war", 0.5);
lda<-LDA(x=dd.tm, k=2, method = "VEM");
dd$rNames = paste(dd$file, dd$line, sep="_");
dd.statement <- data.frame(docs=dd[,"statement"],row.names = c(dd[,"rNames"]));
dd.statement <- DataframeSource(x=dd.statement);
#inspect(Corpus(dd.statement))
dd.tm <- DocumentTermMatrix(Corpus(dd.statement),control = list(removePunctuation = TRUE));
findFreqTerms(dd.tm, 5);
findAssocs(dd.tm, "war", 0.5);
#lda<-LDA(x=dd.tm, k=2, method = "VEM");
lda<-LDA(x=dd.tm, k=2, method = "VEM")
dd.tm<-removeSparseTerms(x=dd.tm, sparse=0.5)
lda<-LDA(x=dd.tm, k=2, method = "VEM");
lda<-LDA(x=dd.tm, k=2, control = list(alpha = 0.1), method = "VEM");
rowTotals <- apply(dd.tm , 1, sum)
rowTotals <- apply(dd.tm , 1, sum);
dd.tm.new   <- dd.tm[rowTotals> 0];
lda<-LDA(x=dd.tm, k=2, control = list(alpha = 0.1), method = "VEM");
View(rowTotals)
dd$rNames = paste(dd$file, dd$line, sep="_");
dd.statement <- data.frame(docs=dd[,"statement"],row.names = c(dd[,"rNames"]));
dd.statement <- DataframeSource(x=dd.statement);
#inspect(Corpus(dd.statement))
dd.tm <- DocumentTermMatrix(Corpus(dd.statement),control = list(removePunctuation = TRUE));
findFreqTerms(dd.tm, 5);
findAssocs(dd.tm, "war", 0.5);
rowTotals <- apply(dd.tm , 1, sum);
dd.tm.new   <- dd.tm[rowTotals> 0];
View(rowTotals)
lda<-LDA(x=dd.tm.new, k=2, control = list(alpha = 0.1), method = "VEM");
lda<-LDA(x=dd.tm, k=2, control = list(alpha = 0.1), method = "VEM");
lda<-LDA(x=dd.tm, k=2);
names(rowTotals)
v<-data.frame(rowTotals)
names(v)
View(v[(rowTotals==0)])
View(v[(v$rowTotals==0)])
View(v[(v$rowTotals==0),])
